\section{\Tool view-centric cost estimator} 
%\junwen{shall we change the title of this section? it seems profiling makes people confused about how we detect the optimization opportunities} \alvin{To me this is a bit confusing as profiling is used to describe both the overall process of finding contributing queries and estimating their costs, and also the `dynamic' profiling used given a test workload}
\label{sec:profile}

There are two key tasks in \Tool's cost estimation. First, given an HTML tag,
\Tool determines which database queries are executed
to generate the data that is rendered through that tag (we refer to them as contributing queries). 
Second, for each HTML tag, \Tool measures the data processing cost needed to render it.

While a web page's load time consists of client side rendering time,
network communication between client and server, 
computation time on the server, and database query cost,
\Tool's estimator currently focuses on database query cost, as query time often contributes a significant portion of the page load time. This is particularly true as the data size increases, and large query results lead to even more computation and rendering time. 
Query time/complexity is also
difficult for developers to estimate, particularly given the ORM abstraction.
As future work, we will incorporate other profiling tools to measure the performance of 
the client code \cite{chromeDev}, network, and server computation as part of \Tool.


\subsection{Identifying contributing queries}
\Tool identifies contributing queries for an HTML tag by statically analyzing
control and data dependencies in the ADG. Given an HTML tag in a view file, 
\Tool first identifies all ADG nodes $N$ that contain the tag's unique ID ---
these nodes contain the Ruby code embedded in the HTML tag. Then, \Tool 
 traces backward along the ADG edges to
identify all query nodes that any node in $N$ has control or data dependence upon. 
All these queries, each identified by its ADG node ID and Ruby source code
location, are considered as contributing queries of this HTML tag. 
For example, in Figure \ref{fig:icq}, tracing dependency edges backward from node 
{\large \textcircled{\small 3}}, which corresponds to HTML tag with id {\tt ti3}, %xx\shan{Junwen to fill}
will identify two contributing queries, nodes {\large \textcircled{\small 1}} and 
{\large \textcircled{\small 2}}.


\Tool further conducts forward dependency checking in the ADG to see how many other
HTML tags each query node contributes to. This number can be used
as a weight in computing the data processing cost of an HTML tag
--- if a query result is used to generate $k$ HTML tags
(e.g., the query node {\large \textcircled{\small 1}} contributes to three HTML tags in Figure \ref{fig:icq}), 
we could attribute $1/k$ of the query cost to each HTML tag if the web developers
choose so (while by default \Tool attributes the complete query cost to each tag).

\subsection{Cost analysis}

%\cong{Is it true that when dynamic profiling, you just replace the static query cost with the actual query running time? If so, I think it's better to introduce static estimation first (including the workflow of how you connect a query cost to a HTML tag), then dynamic.}\shan{Cong, I don't quite see why static
%analysis has to be placed first. The static analysis here is somewhat ad-hoc and more complicated to
%explain, so I feel it is better to discuss the clean dynamic profiling first.}

\Tool offers two modes of cost estimation with or without relying on testing workload.

\subsubsection{Dynamic profiling}
\label{sec:profile_dynamic}

If a testing workload is available, \Tool
will measure the cost of each contributing query during a 
testing run. However, using 
the query execution log from the backend database engine as the testing workload does not work ---
the database engine has no knowledge about frontend Ruby and HTML code, 
and hence does not allow \Tool to connect the statement in the web application that issues the query and the HTML tag uses the query result.

\Tool instead conducts its profiling through a hook API provided by Rails infrastructure, 
{\tt ActiveRecordQueryTrace}. This API allows its hooked code to be called before and after
issuing each SQL query. 
Using this mechanism, \Tool logs the amount of time of each query and the line of source code that
issues this query during the testing run, and attributes the time to the corresponding HTML tags using 
contributing query analysis as discussed above.


\subsubsection{Static estimation}
\label{sec:profile_static}
Since a bottleneck-exposing workload may not be available during in-house testing, \Tool also
uses static analysis to estimate the potential data-processing cost (in terms of its data complexity) to render each HTML tag.
For ease of estimation, \Tool assumes that all tables in the database have the same size $D$. 
%Future work can improve the estimation accuracy by refining this assumption.
Then, for each contributing query, \Tool estimates its complexity (i.e., how its execution
time might increase with $D$) by considering:
(1) the number of times this query might be issued, and
(2) time taken to execute one query instance.

To estimate the first factor, \Tool analyzes loops.
If the query $Q$ is not contained in any loop or is only contained by a loop whose 
iteration number does not increase with $D$, which we refer to as a {\it bounded loop}, 
\Tool then considers $Q$ to be executed for a constant number of times.
Otherwise, \Tool considers $Q$ to be executed for $D^k$ times, with $k$ being the number of {\it unbounded} loops containing $Q$.
To identify the {\it unbounded} loops, \Tool analyzes the bound variable 
of all loops that contain\ $Q$.
If the loop iterates through a set of records returned by an 
{\it unbounded} database query,
\Tool considers the loop to be unbounded.
Specifically, in Rails, a query is {\it unbounded} in all but the following
three cases:
(1) it always returns a single value, like a {\tt SUM} query; 
(2) it always returns a single record by selecting on primary key; %through the {\tt uniq} identifier; 
(3) it always returns a bounded number of records using the {\tt LIMIT} keyword.

To estimate the second factor, \Tool first identifies all the query operators
inside the query $Q$. For example, for the query node {\large \textcircled{\small 2}} in
Figure \ref{fig:icq}, \Tool would identify three query operators 
from the {\tt @user.issues.active.count} statement: 
a {\tt SELECT} to get {\tt issues}, 
another {\tt SELECT} to get {\tt active}, and finally a {\tt count} operator.
For most operators, we estimate its execution complexity to be $O(D)$.
There are a few exceptions:
we consider the complexity of a {\tt JOIN} operator to be $O(D^2)$, and 
the complexity of an operator that explicitly uses index, such as
{\tt find} and {\tt find\_by\_id}, to be constant. 
%\cong{{\tt find} does not explicitly use an index. we may say we assume tables have index on id and foreign key (or have index on every field), and every indexing operator has a constant cost.}

Putting these two factors together gives \Tool the complexity estimation
for one contributing query $Q$. For example, the estimated complexity of the 
query node {\large \textcircled{\small 2}} in Figure \ref{fig:icq} is O($D^3$). If it is
enclosed in an unbounded loop, its complexity would increase to O($D^4$).

\Tool could choose to deliver the above performance information using either a 
detailed text description or a numeric score. The current prototype
uses the latter: it uses the highest 
complexity among all contributing queries as the complexity score of an
HTML tag. For example, in Figure \ref{fig:icq}, 
the complexity score of HTML node {\large \textcircled{\small 3}} is 3, based
on the O($D^3$) complexity estimated for query node  {\large \textcircled{\small 2}}, and
the complexity scores of HTML node {\large \textcircled{\small 4}} and  {\large \textcircled{\small 5}}  are
both 1, based on the O($D$) complexity estimated for query node
{\large \textcircled{\small 1}}.

Of course, this is just a best-effort estimation from \Tool. There are 
several potential
sources of inaccuracy that can be improved by future work.
For example, some database tables may be much larger than others, which
we do not consider; the database can also lower the
query complexity than our estimation due to query optimization.

\iffalse
{\tt where}, 
{\tt all}, 
{\tt group}, 
{\tt join}, 
{\tt find}, 
{\tt includes}, 
{\tt teager\_load}, 
{\tt order}, 
{\tt count}, 
{\tt sum}, 
{\tt maximum}, 
{\tt minimum}, 
{\tt average}, 
{\tt having}, and 
{\tt calculate}. \cong{I think no need to mention query chain. Just say the SQL query type and how we estimate.}
\Tool currently assigns equal cost scores for each such operator.
\fi



 
%And the complexity will be decreased by 1 for limit, first,
